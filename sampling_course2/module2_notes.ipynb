{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aede4b33-1259-42c5-a172-be1997faeb9e",
   "metadata": {},
   "source": [
    "# Module2 summary\n",
    "\n",
    "<h3></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a6d76-59f9-42e3-a633-f80663324fa7",
   "metadata": {},
   "source": [
    "<h2>Introduction to Monte Carlo Methods</h2>\n",
    "<h3>Video:What is MC?</h3>\n",
    "<h3>Using MC for prediction</h3>\n",
    "\n",
    "<h2>Monte Carlo for Control</h2>\n",
    "<h3>Video:</h3>\n",
    "<h3>Video:</h3>\n",
    "<h3>Video:</h3>\n",
    "<h3>Video:</h3>\n",
    "<h2>Exploration Methods for Monte Carlo</h2>\n",
    "<h3>Video:</h3>\n",
    "<h3>Video:</h3>\n",
    "<h3>Video:</h3>\n",
    "<h2>Off policy learning for prediction</h2>\n",
    "<h3>Video: Why does off policy matter</h3>\n",
    "<h3>Video: Importance Sampling</h3>\n",
    "<h3>Video: Off policy MC prediction</h3>\n",
    "<h3>Video: Emma Brunskill!! Batch Reinforcement LEarning</h3>\n",
    "<p>What is special with batch RL? Both for online and offpolicy? BRL is a 2 step process 1) use dataset and train a policy. 2) compute an error to adjust the training policy to account for different distribution on online case. The methods are different from supervised learning but borrow from SL. The traiking data is collected from a behavior policy. Your job is to calulate a new optimal policy and and \"ajustment\" factor if possible, to account for the difference between the older behavior policy and optimal policy. The epsilon greedy concepts are important here.</p>\n",
    "<p></p>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
