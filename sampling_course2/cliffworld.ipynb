{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfc0e1d-7b4f-421b-aa0d-1e864e979284",
   "metadata": {},
   "source": [
    "# Assignment: Policy Evaluation in Cliff Walking Environment\n",
    "\n",
    "Welcome to the Course 2 Module 2 Programming Assignment! In this assignment, you will implement one of the fundamental sample and bootstrapping based model free reinforcement learning agents for prediction. This is namely one that uses one-step temporal difference learning, also known as TD(0). The task is to design an agent for policy evaluation in the Cliff Walking environment. Recall that policy evaluation is the prediction problem where the goal is to accurately estimate the values of states given some policy.\n",
    "\n",
    "### Learning Objectives\n",
    "- Implement parts of the Cliff Walking environment, to get experience specifying MDPs [Section 1].\n",
    "- Implement an agent that uses bootstrapping and, particularly, TD(0) [Section 2].\n",
    "- Apply TD(0) to estimate value functions for different policies, i.e., run policy evaluation experiments [Section 3]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ba0f1-a57c-48e0-857e-91a0780085be",
   "metadata": {},
   "source": [
    "## The Cliff Walking Environment\n",
    "\n",
    "The Cliff Walking environment is a gridworld with a discrete state space and discrete action space. The agent starts at grid cell S. The agent can move (deterministically) to the four neighboring cells by taking actions Up, Down, Left or Right. Trying to move out of the boundary results in staying in the same location. So, for example, trying to move left when at a cell on the leftmost column results in no movement at all and the agent remains in the same location. The agent receives -1 reward per step in most states, and -100 reward when falling off of the cliff. This is an episodic task; termination occurs when the agent reaches the goal grid cell G. Falling off of the cliff results in resetting to the start state, without termination.\n",
    "\n",
    "The diagram below showcases the description above and also illustrates two of the policies we will be evaluating.\n",
    "\n",
    "<img src=\"./cliffwalk.png\" style=\"height:400px\">\n",
    "\n",
    "#### Packages.\n",
    "\n",
    "We import the following libraries that are required for this assignment. We shall be using the following libraries:\n",
    "1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
    "2. numpy: the fundamental package for scientific computing with Python.\n",
    "3. matplotlib: the library for plotting graphs in Python.\n",
    "4. RL-Glue: the library for reinforcement learning experiments.\n",
    "5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.\n",
    "6. Manager: the file allowing for visualization and testing.\n",
    "7. itertools.product: the function that can be used easily to compute permutations.\n",
    "8. tqdm.tqdm: Provides progress bars for visualizing the status of loops.\n",
    "\n",
    "**Please do not import other libraries** this will break the autograder.\n",
    "\n",
    "**NOTE: For this notebook, there is no need to make any calls to methods of random number generators. Spurious or missing calls to random number generators may affect your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f841617-1587-42b8-ad2e-bc16bf97b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in /Users/dc/test_python/coursera_rl/.coursera-rl/lib/python3.11/site-packages (0.0.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /Users/dc/test_python/coursera_rl/.coursera-rl/lib/python3.11/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sampling_module3.ipynb',\n",
       " 'TD0_test.ipynb',\n",
       " 'sampling_module5.ipynb',\n",
       " 'images_module3',\n",
       " 'images',\n",
       " 'sampling_module2.ipynb',\n",
       " 'mc.numbers',\n",
       " 'blackjack.ipynb',\n",
       " 'sampling_module4.ipynb',\n",
       " 'README.md',\n",
       " 'MCMC.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'prog_ass_td0.ipynb',\n",
       " 'mygridworld.ipynb',\n",
       " 'offpolicy.ipynb',\n",
       " 'coursera_notes.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install jdc \n",
    "! pip install numpy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ca6c93-ff02-460a-be8b-b503bdaabc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jdc\n",
    "import numpy as np\n",
    "from rl_glue import RLGlue\n",
    "from Agent import BaseAgent \n",
    "from Environment import BaseEnvironment  \n",
    "from manager import Manager\n",
    "from itertools import product\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53a8972-7cf8-46b0-9397-6204a1856c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Discussion Cell\n",
    "# ---------------\n",
    "\n",
    "# Create empty CliffWalkEnvironment class.\n",
    "# These methods will be filled in later cells.\n",
    "class CliffWalkEnvironment(BaseEnvironment):\n",
    "    def env_init(self, env_info={}):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_start(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_step(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_cleanup(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # helper method\n",
    "    def state(self, loc):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff28468-6cb1-4872-bd6a-f73549786b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CliffWalkEnvironment\n",
    "\n",
    "# ---------------\n",
    "# Discussion Cell\n",
    "# ---------------\n",
    "\n",
    "def env_init(self, env_info={}):\n",
    "        \"\"\"Setup for the environment called when the experiment first starts.\n",
    "        Note:\n",
    "            Initialize a tuple with the reward, first state, boolean\n",
    "            indicating if it's terminal.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note, we can setup the following variables later, in env_start() as it is equivalent. \n",
    "        # Code is left here to adhere to the note above, but these variables are initialized once more\n",
    "        # in env_start() [See the env_start() function below.]\n",
    "        \n",
    "        reward = None\n",
    "        state = None # See Aside\n",
    "        termination = None\n",
    "        self.reward_state_term = (reward, state, termination)\n",
    "        \n",
    "        # AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably \n",
    "        # used with the term \"state\" for our purposes and for this assignment in particular. \n",
    "        # A difference arises in the use of the terms when we have what is called Partial Observability where \n",
    "        # the environment may return states that may not fully represent all the information needed to \n",
    "        # predict values or make decisions (i.e., the environment is non-Markovian.)\n",
    "        \n",
    "        # Set the default height to 4 and width to 12 (as in the diagram given above)\n",
    "        self.grid_h = env_info.get(\"grid_height\", 4) \n",
    "        self.grid_w = env_info.get(\"grid_width\", 12)\n",
    "        \n",
    "        # Now, we can define a frame of reference. Let positive x be towards the direction down and \n",
    "        # positive y be towards the direction right (following the row-major NumPy convention.)\n",
    "        # Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1 \n",
    "        # and max y is then grid_w - 1. So, we have:\n",
    "        # Starting location of agent is the bottom-left corner, (max x, min y). \n",
    "        self.start_loc = (self.grid_h - 1, 0)\n",
    "        # Goal location is the bottom-right corner. (max x, max y).\n",
    "        self.goal_loc = (self.grid_h - 1, self.grid_w - 1)\n",
    "        \n",
    "        # The cliff will contain all the cells between the start_loc and goal_loc.\n",
    "        self.cliff = [(self.grid_h - 1, i) for i in range(1, (self.grid_w - 1))]\n",
    "        \n",
    "        # Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to \n",
    "        # verify that your understanding of the above code is correct for the default case, i.e., where \n",
    "        # height = 4 and width = 12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7867c5-a216-440b-91b3-cd0296dd864e",
   "metadata": {},
   "source": [
    "## *Implement* state()\n",
    "    \n",
    "The agent location can be described as a two-tuple or coordinate (x, y) describing the agentâ€™s position. \n",
    "However, we can convert the (x, y) tuple into a single index and provide agents with just this integer.\n",
    "One reason for this choice is that the spatial aspect of the problem is secondary and there is no need \n",
    "for the agent to know about the exact dimensions of the environment. \n",
    "From the agentâ€™s viewpoint, it is just perceiving some states, accessing their corresponding values \n",
    "in a table, and updating them. Both the coordinate (x, y) state representation and the converted coordinate representation are thus equivalent in this sense.\n",
    "\n",
    "Given a grid cell location, the state() function should return the state; a single index corresponding to the location in the grid.\n",
    "\n",
    "\n",
    "```\n",
    "Example: Suppose grid_h is 2 and grid_w is 2. Then, we can write the grid cell two-tuple or coordinate\n",
    "states as follows (following the usual 0-index convention):\n",
    "|(0, 0) (0, 1)| |0 1|\n",
    "|(1, 0) (1, 1)| |2 3|\n",
    "Assuming row-major order as NumPy does,  we can flatten the latter to get a vector [0 1 2 3].\n",
    "So, if loc = (0, 0) we return 0. While, for loc = (1, 1) we return 3.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730e1c84-7822-46ef-9145-c3b629548264",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CliffWalkEnvironment\n",
    "\n",
    "# -----------\n",
    "# Graded Cell\n",
    "# -----------\n",
    "\n",
    "# Modify the return statement of this function to return a correct single index as \n",
    "# the state (see the logic for this in the previous cell.)\n",
    "def state(self, loc):\n",
    "    # your code here\n",
    "    print(loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a35c5-0ec5-4ef9-867f-a6ecbf0ba627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
