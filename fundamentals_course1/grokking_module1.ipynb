{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ca2eca-3390-4cd8-a620-557884878c8e",
   "metadata": {},
   "source": [
    "<h1>RL Notation and Definitions</h1>\n",
    "<li>Episodic tasks ones which have a limited number of time steps or a natural ending like a pong game</li>\n",
    "<br/>\n",
    "<li>The sequence of time steps in episodic tasks is an episode. A path from start to stop in a game is a single episode. Game play consists of multiple episodes</li>\n",
    "<br/>\n",
    "\n",
    "<li>The sum of rewards in a single episode is a return, denoted $G$</li>\n",
    "<br/>\n",
    "\n",
    "<li>Continuous tasks, unlimited time steps like cart pole balancing. There is no natural stopping point like an episodic task.</li>\n",
    "<br/>\n",
    "\n",
    "<li>Model the environment with a Markov Decision Process. The markov assumption is 1) the current state reflects all the past states. Future states are independent of past states and look at the present state. $P(S_{t+1} | S_t)$. 2) Full observability, the agent knows which state it is in at all times. 3)etc...  A POMDP violates the Markov assumption. Substitute a prior belief for partial observability.  Time delayed dependencies violate MA. A MDP reduces to a system of linear equations if statespace, action space $S$ and $A$ is finite, policy $\\pi$ is fixed and Reward, $R$ and Transition probabilities, $p(s'|s,a)$ are known and environment is fully observable.</li>\n",
    "<br/>\n",
    "\n",
    "<li>The environment is represented by set of variables, $S$, the state space</li>\n",
    "<br/>\n",
    "\n",
    "<li>The state of variables the agent observes at a point of time is an observation</li>\n",
    "<br/>\n",
    "\n",
    "<li>The combination of all possible values of variables is the observation space</li>\n",
    "<br/>\n",
    "\n",
    "<li>Confusing, state space and observation space used interchangably</li>\n",
    "<br/>\n",
    "\n",
    "<li>After a transition the environment emits a new observation</li>\n",
    "<br/>\n",
    "\n",
    "<li>at every state an Agent has a set of actions to choose from. Action space</li>\n",
    "<br/>\n",
    "\n",
    "<li>strictly greater means $\\gt$ there is no equal sign</li>\n",
    "<br />\n",
    "\n",
    "<li>let $\\pi$ be any policy, $\\pi_{*}$ an optimal policy, $V_{*}(s)$ value of state under optimal policy. An optimal policy $\\pi_{*}$ is better than or equal to other nonoptimal policies. There can be more than 1 optimal policy. In gridworld there are multiple paths to the target. There can be multiple shortest paths. </li>\n",
    "<br />\n",
    "\n",
    "\n",
    "<li>The value of a state under an optiimal policy is strictly greater than the corresponding state in a nonoptimal policy. The concept of stricly greater means there can be no equality. This prevents the problem of equal states causing oscillations when searching for optimal policies</li>\n",
    "<br/>\n",
    "<li>The coursera class covers a concept of a greedy policy vs. a equiprobable policy. A greedy policy is an optimal policy compared to teh equiprobable policy. </li>\n",
    "<br />\n",
    "<li></li>\n",
    "<br />\n",
    "<li></li>\n",
    "<br />\n",
    "<li></li>\n",
    "<br />\n",
    "<li></li>\n",
    "<br />\n",
    "<li></li>\n",
    "<br />\n",
    "<li></li>\n",
    "<br />\n",
    "<li></li>\n",
    "<br />\n",
    "<li></li>\n",
    "<br />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c99f47-83b0-4641-bbd5-e5c1674a352c",
   "metadata": {},
   "source": [
    "<h2>3 environments, BW, BSW, FL</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fbc95b-91c4-4813-915c-8e5f7b75bf4c",
   "metadata": {},
   "source": [
    "<img src=\"./images_module1/banditwalk.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba27b5-7888-4690-880c-5cc61c072923",
   "metadata": {},
   "source": [
    "<img src=\"./images_module1/bws.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd4026-467a-467d-9c51-adeac34271bb",
   "metadata": {},
   "source": [
    "<img src=\"./images_module1/fl.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52747a-4581-498f-aae9-b547d567562b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
