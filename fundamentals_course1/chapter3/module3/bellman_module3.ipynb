{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f684ef-d5f4-428d-9469-df7e9c622b53",
   "metadata": {},
   "source": [
    "# MDPs and Random Variables\n",
    "<h4>Random Variables</h4>\n",
    "The general RL problem is formulated with random variables with returns $G$, Rewards $R$, States, $S$\n",
    "and actions $A$. \n",
    "\n",
    "There is an environment and agent. The agent interacts with the environment causing changes in the random variables. \n",
    "\n",
    "<h6>The first step is to add some assumptions. First model sequential behavior with MDPs. The MDP asssumption is only the current state affects future states. We disregard previous states. Assume all the changes from previous states are represented in the current state.  Random Variables model the uncertainity behind the agent and environment. To transition from RV to a real value we take the expectation of a RV and those equations are presented as expectation equations</h6>\n",
    "\n",
    "<h6> Random Variables</h6>\n",
    "Start with RVs  \n",
    "$M(S,R,A,G)$ where we optimize the total return G over all states, actions. G to be defined later is some function of R, S, and parameters we can choose to add like $\\gamma$ over the set of actions. This notation is because we choose MDPs as our model and we choose gamma as an artifical construct to make the math converge. There are other models which don't use this notation or set of assumptions. \n",
    "\n",
    "Add time and limit the previous states to current state with markov assumption\n",
    "$M(S_t, S_{t+1}, A_t, R_{t+1}, G_t)$\n",
    "\n",
    "\n",
    "<h6> MDP</h6>\n",
    "Combine states, rewards with actions and transition probabiliies to define MDPs\n",
    "\n",
    "\n",
    "# Policies \n",
    "<h6>Policies are distributions over actions for each possible state</h6>\n",
    "$\\pi=p(a|s)$\n",
    "<h4>Bellman Equation Assumptions</h4>\n",
    "<br />\n",
    "<li>environment must be Markov Decision process and the markov assumption means future state depends on ONLY the current state and not more historical states </li>\n",
    "\n",
    "$P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_0, a_0, â€¦, s_t, a_t)$\n",
    "<br />\n",
    "<br />\n",
    "<li>transition probabilities  $P(s' | s, a)$ and reward distributions $R(s, a)$ are fixed over time</li>\n",
    "<br />\n",
    "<h2>Bellman Expectation equations. </h2>\n",
    "These are solutions to the RV equations for MDP. We remove the RV by taking expectations. \n",
    "\n",
    "<h6>Bellman equation state value</h6>\n",
    "$V^\\pi(s) = \\mathbb{E}_\\pi \\left[ r(s,a) + \\gamma V^\\pi(s') \\right]$\n",
    "<h6>Bellman equation action value</h6>\n",
    "$Q^\\pi(s,a) = \\mathbb{E} \\left[ r(s,a) + \\gamma \\mathbb{E}_{a{\\prime}\\sim\\pi} \\left[ Q^\\pi(s',a') \\right] \\right]$\n",
    "\n",
    "<h2>Bellman Optimality equations. </h2>\n",
    "<h6>Bellman Optimality state value</h6>\n",
    "$V(s) = \\max_a \\mathbb{E} \\left[ r(s,a) + \\gamma V^(s') \\right]$\n",
    "<h6>Bellman Optimality action value</h6>\n",
    "$Q(s,a) = \\mathbb{E} \\left[ r(s,a) + \\gamma \\max_{a'} Q^(s',a') \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508084d-d815-44c4-9929-6b2ae589c2d3",
   "metadata": {},
   "source": [
    "<h4>Bellman problem setup</h4>\n",
    "<li>\t2 states: s_1, s_2 </li>\n",
    "<li>\t2 actions: a_1, a_2 </li>\n",
    "<h4>\tRewards:            </h4>\n",
    "\t<li>\tr(s_1, a_1) = 5 </li>\n",
    "\t<li>\tr(s_1, a_2) = 10 </li>\n",
    "\t<li>\tr(s_2, a_1) = 0 </li>\n",
    "\t<li>\tr(s_2, a_2) = 1 </li>\n",
    "<h4>\tTransition: deterministic, action brings you to the same state (no moving around).</h4>\n",
    "<h4>\tDiscount factor \\gamma = 0.9. </h4>\n",
    "\t<h4>\tPolicy $\\pi$: </h4>\n",
    "\t<li>   In s_1, pick a_1 with 80%, a_2 with 20%. </li>\n",
    "\t<li>\tIn s_2, pick a_1 with 50%, a_2 with 50%. </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252e833-3e91-4119-847b-f00871abd1a2",
   "metadata": {},
   "source": [
    "# Example Bellman Expectation\n",
    "$V^{\\pi}(s_1)$=$ 0.8 \\times (5 + 0.9 V^\\pi(s_1)) + 0.2 \\times (10 + 0.9 V^\\pi(s_1))$\n",
    "\n",
    "<h6>Simplify:</h6>\n",
    "\n",
    "$V^{\\pi}(s_1)$ = $(0.8 \\times 5 + 0.2 \\times 10) + (0.8 \\times 0.9 + 0.2 \\times 0.9)V^\\pi(s_1)$\n",
    "$V^{\\pi}(s_1)$ = $(4 + 2) + (0.9)V^\\pi(s_1)$\n",
    "$V^{\\pi}(s_1)$ = $6 + 0.9 V^\\pi(s_1)$\n",
    "\n",
    "\n",
    "$V^\\pi(s_1) = \\frac{6}{1 - 0.9} = 60$\n",
    "<h4>The other state</h4>\n",
    "$V^{\\pi}(s_2)=0.5 \\times (0 + 0.9 V^\\pi(s_2)) + 0.5 \\times (1 + 0.9 V^\\pi(s_2))$\n",
    "<br />\n",
    "$V^{\\pi}(s_2)=0.5 +  0.9 V^\\pi(s_2))$\n",
    "<br />\n",
    "$V^{\\pi}(s_2)*(1-0.9)=0.5 $\n",
    "<br />\n",
    "$V^{\\pi}(s_2)= \\frac{.5}{0.1} = 5 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa816697-d3df-4479-a2d4-5097d310e136",
   "metadata": {},
   "source": [
    "# Example Bellman Optimality\n",
    "<h4>This is the optimal policy</h4>\n",
    "$V^{\\pi}(s_1) = \\max_a \\left( r(s_1,a) + \\gamma V^{\\pi}(s_1) \\right)$\n",
    "<br />\n",
    "\n",
    "<h4>For $a_1$:</h4>\n",
    "$V^{\\pi}(s_1|a_1) = \\left( r(s_1,a) + \\gamma V^{\\pi}(s_1) \\right) = 5+0.9*V^{\\pi}(s_1)$\n",
    "<br />\n",
    "\n",
    "<h4>For $a_2$:</h4>\n",
    "\n",
    "$V^{\\pi}(s_1|a_2) =  \\left( r(s_1,a) + \\gamma V^{\\pi}(s_1) \\right) = 10 + 0.9*V^{\\pi}(s_1)$\n",
    "<br />\n",
    "\n",
    "<h4>Pick $a_2$ since it is bigger than $a_1$t</h4>\n",
    "$V^{\\pi}(s_1|a_2) = 10 + 0.9*V^{\\pi}(s_1)$\n",
    "<br />\n",
    "\n",
    "$V^{*}(s_1) (1-0.9) = 10 $\n",
    "<br />\n",
    "$V^{*}(s_1)  = \\frac{10}{0.1} = 100 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e33900-5c1d-4370-b09a-05a8557a3615",
   "metadata": {},
   "source": [
    "Exercise 3.8 Suppose $\\gamma$ = 0.5 and the following sequence of rewards is received R1 = -1, R2 =2,R3 =6,R4 =3,and R5 =2,with T =5. WhatareG0,G1,...,G5? Hint: Work backwards.\n",
    "\n",
    "$G_t = \\sum_{k=t+1}^T \\gamma^{k-t-1} R_k$\n",
    "<br />\n",
    "$G_t = \\sum_{k=t+1}^5 \\gamma^{k-t-1} R_k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03706b9-2906-4d1e-8bbd-d5617accc408",
   "metadata": {},
   "source": [
    "$G_0$ = $R_1 + \\gamma G_1 = -1 + 0.5 * 6 = 2$ \n",
    "<br />\n",
    "$G_1$ = $R_2 + \\gamma G_2 = 2 + 0.5 * 8 = 6 $ \n",
    "<br />\n",
    "$G_2$ = $R_3 + \\gamma G_3 = 6 + 0.5 * 4 = 8$ \n",
    "<br />\n",
    "$G_3$ = $R_4 + \\gamma G_4 = 3 + 0.5*2 = 4 $ \n",
    "<br />\n",
    "$G_4$ = $R_5 + \\gamma G_5 = R_5 = 2$ \n",
    "<br />\n",
    "$G_5$ = 0; terminal states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa884e8-3952-46c9-afec-d929f0644653",
   "metadata": {},
   "source": [
    "Exercise 3.9 Suppose $\\gamma$= 0.9 and the reward sequence is R1 = 2 followed by an infinite sequence of 7s. What are G1 and G0?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc8f6e-84e5-4394-b757-04791798930f",
   "metadata": {},
   "source": [
    "<img src=\"./diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7dbf49-c73f-4b54-a86c-cb4019e409cd",
   "metadata": {},
   "source": [
    "<h4>Law of total expectation</h4>\n",
    "$E[S] = \\sum_M P(M) E[S*M]$\n",
    "\n",
    "$E_\\pi[R_{t+1} + \\gamma V_\\pi(S_{t+1}| S_t=s)] = \\sum \\pi(a|s) E[R_{t+1} + \\gamma V_\\pi(S_{t+1}| S_t=s, A_t=a]$\n",
    "<h4>Given </h4>\n",
    "let r(s,a) be the expected reward $r(s,a)\\doteq E[R_t|S_{t-1}=s, A_{t-1}=a] = \\sum_r r \\sum_{s'} p(s',r|s,a)$\n",
    "<h4>all of the following are true from the test question in the end:</h4>\n",
    "\n",
    "$q_*(s,a)=r(s,a)+\\gamma \\sum_{s'}p(s'|s,a)max_{a'}q_{*}(s',a')$\n",
    "<br/>\n",
    "$v_*(s)=max_a[r(s,a)+\\gamma \\sum_{s'}p(s'|s,a)v_*(s')]$\n",
    "<br/>\n",
    "$v_\\pi(s)=\\sum_a \\pi(a|s)[r(s,a)+\\gamma\\sum_{s'}p(s'|a,a)v_\\pi(s')]$\n",
    "<br/>\n",
    "$q_{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'}\\sum_{a'}p(s'|s,a)\\pi(a'|s')q_{\\pi}(s',a')$\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc258441-175f-4852-adcd-bd6233659714",
   "metadata": {},
   "source": [
    "<h1>Gridworld</h1>\n",
    "\n",
    "<h4>The bellman equations for gridworld</h4>\n",
    "\n",
    "$v_\\pi(s) = \\sum \\pi(a|s)[r + \\gamma v_\\pi(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47110cf-30e1-4e25-af22-e2dda7224a14",
   "metadata": {},
   "source": [
    "<img src=\"gridworld.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122c6a0-2208-413c-976e-aad0d6fa0b06",
   "metadata": {},
   "source": [
    "<h1>Pole</h1>\n",
    "<img src=\"pole.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a5e24-e563-443a-b688-9b600a018cb7",
   "metadata": {},
   "source": [
    "\n",
    "Exercise 3.6 Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n",
    "$G_t= R_{t+1}+\\gamma*R_{t+2}+\\gamma^2*R_{t+3}+...\\gamma^{T-1}*R_{T}= -1\\gamma^{T}$\n",
    "<br />\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73657c57-1b59-4b26-849b-5d9ece502701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
