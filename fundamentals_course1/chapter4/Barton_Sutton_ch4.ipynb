{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b84df0-a968-4f21-89ab-6fd667cfa193",
   "metadata": {},
   "source": [
    "<h1>Dynamic Programming</h1>\n",
    "<h4>DP compute optimal policies given perfect model of environment as MDP. DP is used to calculate the value fns to search for policies. </h4>\n",
    "<h4>DP requires exhaustive search over the entire state space. In Jack's car rental program the loop looks at all 400 states many times</h4>\n",
    "<h4>Exhaustive search is the hand wavy argument we can find the optimal policy</h4>\n",
    "<h4>Can obtain optimal policy $\\pi_*$ from optimal value fn $v_*$</h4>\n",
    "<h4>A value function is:</h4>\n",
    "<p>$v_\\pi(s) \\doteq E_\\pi[G_t|S_t=s]$</p>\n",
    "<p>$= E_{\\pi}[R_{t+1}+\\gamma G_{t+1} | S_t=s]$</p>\n",
    "<p>$= E_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1}) | S_t=s] $</p>\n",
    "<p>$=\\sum\\limits_a \\pi(a|s) \\sum \\limits_{s', r} p(s',r|s,a)[r'+\\gamma v_{\\pi}(s') ]$</p>\n",
    "<h4>If the environment is known, ie gridworld, then the above equation becomes a system of linear equations</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8239d967-943a-45f4-96a8-05e397a8b446",
   "metadata": {},
   "source": [
    "<h4>3 state Example Gridworld</h4>\n",
    "To specify need states, actions, rewards, transition probabilities,  p(s',r|s,a)\n",
    "<li>3 state GW, $S={s_1,s_2,s_3}$</li>\n",
    "<li>single action per state, </li>\n",
    "<li>Transition probability and reward below</li>\n",
    "$s_1, s_2, P=1, R=1$\n",
    "<br/>\n",
    "$s_2,s_3, P=1, R=2$\n",
    "<br/>\n",
    "$s_3, s_3, P=1, R=0$\n",
    "<br/>\n",
    "<h4>Linear equations assume $\\gamma=0.9$</h4>\n",
    "$V(state) = Reward + \\gamma(next state)$\n",
    "<br/>\n",
    "$V(s_1)=1+0.9V(s_2)$\n",
    "<br/>\n",
    "$V(s_2)=2+0.9V(s_3)$\n",
    "<br/>\n",
    "$V(s_3)=0+0.9V(s_3)$\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278502c7-1435-4e32-8122-b5229a01b416",
   "metadata": {},
   "source": [
    "<h4>The full policy iteration shown as a tuple(state, optimal action, value). This is a simple example because we can only go from s1->s2, then s2->s3 and s3->s3 forever.  </h4>\n",
    "$(s_1,0,2.8)$->$(s_2,0,2.0)$->$(s_3,0,0.)$\n",
    "<br/>\n",
    "What is the notation for policy iteration? \n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da7664-2799-4bf7-8191-0b37bd6ec4e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccda1c06-206d-45f6-a884-5b905c78e599",
   "metadata": {},
   "source": [
    "<h4>Example 4.1 Gridworld</h4>\n",
    "<img src=\"./grid.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a1850-adc5-4ae5-8339-889946f02620",
   "metadata": {},
   "source": [
    "The nonterminal states are S = {1, 2, . . . , 14}. There are four actions possible in each state, A = {up, down, right, left}, \n",
    "which deterministically cause the corresponding state transitions, except that actions that would take the agent off the grid in fact \n",
    "leave the state unchanged. Thus, for instance, p(6,-1|5,right) = 1, p(7,-1|7,right) = 1, and p(10,r|5,right) = 0 for all r $\\in$ R. This is an undiscounted, episodic task. The reward is -1 on all transitions until the terminal state is reached. The terminal state is shaded in the figure (although it is shown in two places, it is formally one state). The expected reward function is thus r(s, a, s0) = -1 for all states s, s0 and actions a. Suppose the agent follows the equiprobable random policy (all actions equally likely). The left side of Figure 4.1 shows the sequence of value functions {vk} computed by iterative policy evaluation. The final estimate is in fact $v_{\\pi}$, which in this case gives for each state the negation of the expected number of steps from that state until termination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59bfa57-ea22-4316-9570-dbf36fa57c76",
   "metadata": {},
   "source": [
    "Exercise 4.1 In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_{\\pi} (11, down)$? What is $q_{\\pi}(7,down)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9745a-e498-4d25-9416-22c985587e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "079d8af5-eab6-4c29-bfff-68ef8d137d0c",
   "metadata": {},
   "source": [
    "Exercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is v⇡(15) for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is v⇡(15) for the equiprobable random policy in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30eecf7-119a-42c0-9f21-3cad9c7f2a59",
   "metadata": {},
   "source": [
    "Exercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5) for the action- valuefunctionq⇡ anditssuccessiveapproximationbyasequenceoffunctionsq0,q1,q2,...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4907e01-5636-49a7-9089-81662333172a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d51edd9d-a769-4198-b26e-aacf49111a40",
   "metadata": {},
   "source": [
    "Example 4.2: Jack’s Car Rental Jack manages two locations for a nationwide car\n",
    "rental company. Each day, some number of customers arrive at each location to rent cars.\n",
    "If Jack has a car available, he rents it out and is credited 10 dollars by the national company.\n",
    "If he is out of cars at that location, then the business is lost. Cars become available for\n",
    "renting the day after they are returned. To help ensure that cars are available where\n",
    "they are needed, Jack can move them between the two locations overnight, at a cost of\n",
    "2 dollars per car moved. We assume that the number of cars requested and returned at each\n",
    "location are Poisson random variables, meaning that the probability that the number is n is $\\frac{\\lambda^{n}}{n!}e^{-\\lambda}$, where $\\lambda $ is the expected number. Suppose $\\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a\n",
    "maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\\gamma$ = 0.9 and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cars moved between the two locations overnight. Figure 4.2 shows the sequence of policies found by policy iteration starting from the policy that never moves any cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cbe1fe-1f46-4734-b4a2-275b61d6ad11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
