{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b8adce-d0ef-4ee6-9c53-269b69a12d17",
   "metadata": {},
   "source": [
    "<h2>Module 2 An Introduction to Sequential Decision Making</h2>\n",
    "<h4>SB 2.1-2.7 Doesnt include gradient bandit methods which should be includedm</h4>\n",
    "<h3>Video: Sequential Decision making with evaluative feedback</h3>\n",
    "<p>An agent makes it's own training data by interacting with the environment. The decision the agent makes, choosing between k-choices. Define rewards, time-steps and values. The result from the choice is a value. Sometimes there is a reward when choosing. The amount of the reward can vary. The result of an action, pulling one of the k arms is a value. The agent owns the action-value pairs for each arm. The value is defined as $q_{*}$ </p>\n",
    "<p>The value is the expected reward per action $q_{*}(a) \\doteq E[R_t|A_t=a] \\forall a \\in {1,2,...k} = \\sum_{r} p(r|a)*r $</p>\n",
    "<p>This action-value pairing is one of the key concepts and conventions in RL. The evaluation of the expectation gives the reward. The C video correctly states evaluating the expectation for the continuous case is an integral. They leave out the detail the integral isn't a Riemann integral but a lebesque integral. This is a point of confusion for many. </p>\n",
    "\n",
    "<p>The objective is to maximixe the reward $argmax_a q_{*}(a)$ The asterik means the true reward. When we evaluate the expectation we are calculating the mean which is an estimate of the true max. We don't know what the truth is unless we do a simulation and have control over the environment. The mean is the expected reward over all randomness in the environment. The estimated mean is $Q_t(a)$ and is an approximation to $q_{*}$</p>\n",
    "\n",
    "<p>The module2 video is not clear on the difference between optimal and ground truth vs. an estimate over all randomness and the difference in notation. It adds a verbose example of a medical doctor in a clinical trial and tries to define thorugh visualization the terms reward an and trials. The notation isn't clear at this point.  </p>\n",
    "\n",
    "<h3>Video: Learning action values</h3>\n",
    "<p>Video says use sample average to compute expectation of q. The correct notation would be to use $Q_t$ and separate this from the optimal $q_{*}$. For the discrete case we can define $Q_{t}(a) \\doteq \\frac{sum\\ of\\ rewards\\ when\\ a\\ taken\\ prior\\ to\\ t}{number\\ of\\ times\\ a\\ taken\\ prior\\ to\\ t}$</p>\n",
    "<p>sample average = $\\frac{\\sum_{i=1}^{t-1} R_i}{t-1}$<. This notation is sloppy and refers to a single action a. There are many actions prior to t. And there are many choices for the next action at time t=t. We count the sum of rewards when taking action a divided by the number of times action a is taken. In a k bandit environment we isolate the pulls on one arm and sum the rewards from arm 1 divided by the number of times arm 1 is pulled,  up to t-1. /p>\n",
    "<p>A simlation of a k bandit using montecarlo to converge to the ground truth frequency is xxxx. </p>\n",
    "<p>There is a distinction between selecting the action with the highest reward ,called a greedy policy. The greedy policy is exploitation. Selecting the non-argmax arm is exploration. Can't always select the highest arm without exploration. Different policies have different balances between exploration and exploitation. </p>\n",
    "\n",
    "<p>learning about action values can be called exploration. But it is at the expense of exploitation meaning you have to suffer a smaller immediate reward. </p>\n",
    "<h3>Video: Estimating action values incrementally.</h3>\n",
    "<p>Talks about ad serving system and modeling as k-armed bandit problem. He covers onpolicy and offpolicy concepts without using the terminology. Very confusing. Video says how to create a system for ad optimization modeling as k-armed bandit wo storing the clicks? </p>\n",
    "<p>Better to use the openbandit dataset and github and run experiments here. </p>\n",
    "<p>He covers 3 concepts 1) how to forumlate an incremenatal update rule for estimating action-value with sample average. 2) how this is a case of the general update rule, and 3) how to modify the general update rule for nonstationary action-values. </p>\n",
    "<p>To avoid storing all the previous data, rewrite the increment update rule recursively: $$</p>\n",
    "<h1>learning objectives</h1>\n",
    "<h2>Lesson 1 k-armed bandits </h2>\n",
    "<li>Define reward</li>\n",
    "<p>The reward is the evaluation of the expecctaion of the action-value definition. NOTE: did not call this an action-value function. /p>\n",
    "<li>Understand the temporal nature of the bandit problem</li>\n",
    "<p>For t steps, in definition of action-value. </p>\n",
    "<li>Define k-armed bandit</li>\n",
    "<p>Where an agent has a choice between k arms</p>\n",
    "<li>Define action-values</li>\n",
    "$Q_{t}(a) \\doteq E[R_t | A_t=a]$\n",
    "<h2>Lesson 2 estimating action values</h2>\n",
    "Compute the expectation for the discrete case with sample average\n",
    "\n",
    "<li>Define action-value estimation methods</li>\n",
    "<li>Define exploration and exploitation</li>\n",
    "<li>Select actions greedily using an action-value function</li>\n",
    "<li>Define online learning</li>\n",
    "<li>Understand a simple online sample-average action-value estimation method</li>\n",
    "<li>Define the general online update equation</li>\n",
    "<li>Understand why we might use a constant stepsize in the case of non-stationarity</li>\n",
    "<h2>Lesson 3 exploration vs exploitation</h2>\n",
    "<li>Define epsilon-greedy</li>\n",
    "<li>Compare the short-term benefits of exploitation and the long-term benefits of exploration</li>\n",
    "<li>Understand optimistic initial values</li>\n",
    "<li>Describe the benefits of optimistic initial values for early exploration</li>\n",
    "<li>Explain the criticisms of optimistic initial values</li>\n",
    "<li>Describe the upper confidence bound action selection method</li>\n",
    "<li>Define optimism in the face of uncertainty</li>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0861449-688e-417f-9e35-3c2029e3956e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
